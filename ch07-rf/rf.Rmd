---
title: "Random Forest"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r}
library(tidyverse)
library(tree)
```


Slides: p1-p20, p37-p43

# Tree

We only illustrate it via a classification tree. Much of the followings are also true for regression tree.

```{r}
library(kernlab)  # for the data spam
data(spam)
```


```{r}
# classification tree
tree_spam <- tree(type ~ ., data = spam)
plot(tree_spam, type = "uniform")
text(tree_spam, pretty = 1, all = TRUE, cex = 0.7)
```

```{r}
# regression tree
tree_mtcars <- tree(mpg ~ ., data = mtcars)
plot(tree_mtcars, type = "uniform")
text(tree_mtcars, pretty = 1, all = TRUE, cex = 0.7)
```


```{r}
# a tree is not very stable wrt change of data
tree_spam <- tree(type ~ ., data = spam[sample.int(nrow(spam), 1000),])
plot(tree_spam, type = "uniform")
text(tree_spam, pretty = 1, all = TRUE, cex = 0.7)
```

# Random forest

In R, there are packages like `randomForest` and `ranger` to perform random forest. However, we want to implement it from scratch.

The main idea behind random forest is to resample the dataset by 
- sampling the rows with replacement and 
- selecting the columns randomly.


For the `spam` data, suppose we want to predict the class of a random observation.
(We randomly draw an observation from the original data and pretend that we do not know its class)

```{r}
set.seed(141)
new_data <- spam %>% sample_n(1) %>% select(-type)
```

With the whole tree, we could do
```{r}
tree_spam <- tree(type ~ ., data = spam)
predict(tree_spam, new_data)
predict(tree_spam, new_data, type = "class")
```
that gives the probability of nonspam about 0.81.


However it is known that a single tree is not predictive and stable. We want to use bootstrap to increase the predictability.


```{r}
# select row randomly with replacement
spam_boot <- tree(type ~ ., data = spam[sample.int(nrow(spam), replace = TRUE), ])
predict(spam_boot, new_data)
predict(spam_boot, new_data, type = "class")
```

```{r}
# select row randomly with replacement and select columns randomly
all_col_names <- names(spam)[1:57] 
m <- 8   # = round(sqrt(ncol(spam)))
col_names <- c("type", sample(all_col_names, m))
spam_boot <- tree(type ~ ., data = spam[sample.int(nrow(spam), replace = TRUE), col_names])
plot(spam_boot, type = "uniform")
text(spam_boot, pretty = 1, all = TRUE, cex = 0.7)
predict(spam_boot, new_data)
predict(spam_boot, new_data, type = "class")
```


```{r}
r <- 500
n <- nrow(spam)
m <- 8  # about sqrt(57)
all_col_names <- names(spam)[1:57]  # skip "type"

probs <- map_dbl(seq_len(r), function(i) {
  col_names <- c("type", sample(all_col_names, m))
  spam_boot <- spam[sample(n, n, replace = TRUE), col_names]
  tree_spam_boot <- tree(type ~ ., spam_boot)
  # we only need the probability of spam, because the sum of the two values is always 1
  predict(tree_spam_boot, new_data)[2]
})
```


There are two ways to yield the final predicted class, either by consensus or by averaging probabilities. Either way, we need a baseline to compare with -
using the prior proportion as the baseline is a simplest way (though may not be the best way). One may also use CV to select the baseline.

```{r}
(baseline <- mean(spam$type == "spam"))
```

- prediction by consensus

```{r}
mean(probs > baseline)
```
Since more than 50% of the trees predicted `spam`, by consensus, the predicted class for the new data is spam.

- prediction by averaging probabilities

```{r}
mean(probs)
```

The average probability across all trees is about 0.44 > baseline so the predicted class for `new_data` is "spam". For this new data, we have the same prediction using average probability.


In general, it is more stable to use average probability rather than consensus.


# `rpart` package


In general, we want to keep each tree as large as possible

```{r, eval = FALSE}
r <- 500
m <- 8
n <- nrow(spam)
all_col_names <- names(spam)[1:57]  # skip "type"

probs <- map_dbl(seq_len(r), function(i) {
  col_names <- c("type", sample(all_col_names, m))
  spam_boot <- spam[sample(n, n, replace = TRUE), col_names]
  # make larger trees by `tree.control`
  tree_spam_boot <- tree(type ~ ., spam_boot, control = tree.control(n, mindev = 0.00001))
  # we only need the probability of spam, because the sum of the two values is always 1
  predict(tree_spam_boot, new_data)[2]
})
```

```{r}
library(rpart)

probs <- map_dbl(seq_len(r), function(i) {
  col_names <- c("type", sample(all_col_names, m))
  spam_boot <- spam[sample(n, n, replace = TRUE), col_names]
  # make larger trees by using rpart
  tree_spam_boot <- rpart(type ~ ., spam_boot, control = rpart.control(cp = 0))
  # we only need the probability of spam, because the sum of the two values is always 1
  predict(tree_spam_boot, new_data)[2]
})
```

```{r}
# better result!?
mean(probs)
```

# A regression tree example

```{r}
r <- 500
m <- 8
n <- nrow(mtcars)
all_col_names <- names(mtcars)[2:10] # skip "type"
one_row <- mtcars %>% sample_n(1)
new_data <- one_row %>% select(-mpg) # pretend that it is a new data

yhat <- map_dbl(seq_len(r), function(i) {
  col_names <- c("mpg", sample(all_col_names, m))
  mtcars_boot <- mtcars[sample(n, n, replace = TRUE), col_names]
  # make larger trees by using rpart
  tree_mtcars_boot <- rpart(mpg  ~ ., mtcars_boot, control = rpart.control(cp = 0))
  predict(tree_mtcars_boot, new_data)
})
```

```{r}
mean(yhat)
```
```{r}
# the true value
one_row$mpg
```


# `ranger` package

In practice, we will use `ranger` for random forest because it is way faster.

```{r}
library(ranger)

fit <- ranger(type ~ ., data = spam, probability = TRUE)
```

```{r}
new_data <- spam %>% sample_n(1) %>% select(-type)
predicted <- predict(fit, new_data, type = "response")
predicted$predictions
```


# Confidence interval

To construct a CI, you may be thinking of

```{r}
quantile(probs, c(0.025, 0.975))
```

This confidence interval is essentially the bootstrap percentile interval for a tree model which randomly selects $m$ predictors. It is not reliable because the sampling of the columns introduces extra variability. A more sophicated way is to make use of the Jackknife, see https://arxiv.org/pdf/1311.4555.pdf

Well, it is too hard!? Use the package `ranger`!

```{r}
fit <- ranger(type ~ ., data = spam, probability = TRUE, keep.inbag = TRUE)
predicted <- predict(fit, new_data, type = "se")
```

```{r}
predicted$predictions
predicted$se
```

```{r}
predicted$predictions[2] + c(-1, 1) * 1.96 * predicted$se[2]
```

